{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Load + Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "            transform=transforms.Compose([      \n",
    "                transforms.ToTensor(),           \n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('../data', train=False,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ]))\n",
    "\n",
    "def train_valid_split(train_dataset, fraction):\n",
    "    np.random.random(148)\n",
    "    \n",
    "    all_indices = list(range(len(train_dataset)))\n",
    "    np.random.shuffle(all_indices)\n",
    "    all_indices = all_indices[:int(len(all_indices)*fraction)]\n",
    "    \n",
    "    digits = {}\n",
    "    for i in all_indices:\n",
    "        label = train_dataset[i][1]\n",
    "        if label in digits:\n",
    "            digits[label].append(i)\n",
    "        else:\n",
    "            digits[label] = [i]\n",
    "    \n",
    "    subset_indices_train = []\n",
    "    subset_indices_valid = []\n",
    "    \n",
    "    for digit in digits:\n",
    "        indices = digits[digit]\n",
    "        total = len(indices)\n",
    "        np.random.shuffle(indices)\n",
    "        subset_indices_valid.extend(indices[:int(0.15*total)])\n",
    "        subset_indices_train.extend(indices[int(0.15*total):int(total)])\n",
    "\n",
    "    assert(len(subset_indices_train) + len(subset_indices_valid) == len(all_indices))\n",
    "    return subset_indices_train, subset_indices_valid\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    Build the best MNIST classifier.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3), stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.norm = nn.BatchNorm2d(64)\n",
    "        self.dropout1 = nn.Dropout2d(0.15)\n",
    "        self.dropout2 = nn.Dropout2d(0.15)\n",
    "        self.fc1 = nn.Linear(1600, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    '''\n",
    "    This is your training function. When you call this function, the model is\n",
    "    trained for 1 epoch.\n",
    "    '''\n",
    "    model.train()  \n",
    "    losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()               \n",
    "        output = model(data)              \n",
    "        loss = F.nll_loss(output, target) \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()                  \n",
    "        optimizer.step()                   \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.sampler),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))     \n",
    "    return np.average(losses)\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_num = 0\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_num += len(data)\n",
    "    test_loss /= test_num\n",
    "    print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, test_num, 100. * correct / test_num))\n",
    "    return (100. * correct / test_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3192 (0%)]\tLoss: 2.323927\n",
      "Train Epoch: 1 [640/3192 (20%)]\tLoss: 1.533545\n",
      "Train Epoch: 1 [1280/3192 (40%)]\tLoss: 0.717420\n",
      "Train Epoch: 1 [1920/3192 (60%)]\tLoss: 0.506871\n",
      "Train Epoch: 1 [2560/3192 (80%)]\tLoss: 0.289916\n",
      "\n",
      "Accuracy: 526/558 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/3192 (0%)]\tLoss: 0.165952\n",
      "Train Epoch: 2 [640/3192 (20%)]\tLoss: 0.183172\n",
      "Train Epoch: 2 [1280/3192 (40%)]\tLoss: 0.370768\n",
      "Train Epoch: 2 [1920/3192 (60%)]\tLoss: 0.212875\n",
      "Train Epoch: 2 [2560/3192 (80%)]\tLoss: 0.137853\n",
      "\n",
      "Accuracy: 525/558 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/3192 (0%)]\tLoss: 0.086240\n",
      "Train Epoch: 3 [640/3192 (20%)]\tLoss: 0.094332\n",
      "Train Epoch: 3 [1280/3192 (40%)]\tLoss: 0.098435\n",
      "Train Epoch: 3 [1920/3192 (60%)]\tLoss: 0.118852\n",
      "Train Epoch: 3 [2560/3192 (80%)]\tLoss: 0.126426\n",
      "\n",
      "Accuracy: 538/558 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/3192 (0%)]\tLoss: 0.059022\n",
      "Train Epoch: 4 [640/3192 (20%)]\tLoss: 0.034134\n",
      "Train Epoch: 4 [1280/3192 (40%)]\tLoss: 0.013709\n",
      "Train Epoch: 4 [1920/3192 (60%)]\tLoss: 0.043976\n",
      "Train Epoch: 4 [2560/3192 (80%)]\tLoss: 0.035618\n",
      "\n",
      "Accuracy: 538/558 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/3192 (0%)]\tLoss: 0.033720\n",
      "Train Epoch: 5 [640/3192 (20%)]\tLoss: 0.013083\n",
      "Train Epoch: 5 [1280/3192 (40%)]\tLoss: 0.029590\n",
      "Train Epoch: 5 [1920/3192 (60%)]\tLoss: 0.067225\n",
      "Train Epoch: 5 [2560/3192 (80%)]\tLoss: 0.053959\n",
      "\n",
      "Accuracy: 543/558 (97%)\n",
      "\n",
      "Training Set (0.0625 Subset):\n",
      "\n",
      "Accuracy: 3159/3192 (99%)\n",
      "\n",
      "Test Set (0.0625 Subset):\n",
      "\n",
      "Accuracy: 9712/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/6381 (0%)]\tLoss: 2.312631\n",
      "Train Epoch: 1 [640/6381 (10%)]\tLoss: 1.723325\n",
      "Train Epoch: 1 [1280/6381 (20%)]\tLoss: 1.145520\n",
      "Train Epoch: 1 [1920/6381 (30%)]\tLoss: 0.367385\n",
      "Train Epoch: 1 [2560/6381 (40%)]\tLoss: 0.317788\n",
      "Train Epoch: 1 [3200/6381 (50%)]\tLoss: 0.158794\n",
      "Train Epoch: 1 [3840/6381 (60%)]\tLoss: 0.358371\n",
      "Train Epoch: 1 [4480/6381 (70%)]\tLoss: 0.168342\n",
      "Train Epoch: 1 [5120/6381 (80%)]\tLoss: 0.129077\n",
      "Train Epoch: 1 [5760/6381 (90%)]\tLoss: 0.140965\n",
      "\n",
      "Accuracy: 1071/1119 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/6381 (0%)]\tLoss: 0.305538\n",
      "Train Epoch: 2 [640/6381 (10%)]\tLoss: 0.122985\n",
      "Train Epoch: 2 [1280/6381 (20%)]\tLoss: 0.138735\n",
      "Train Epoch: 2 [1920/6381 (30%)]\tLoss: 0.253540\n",
      "Train Epoch: 2 [2560/6381 (40%)]\tLoss: 0.289659\n",
      "Train Epoch: 2 [3200/6381 (50%)]\tLoss: 0.046628\n",
      "Train Epoch: 2 [3840/6381 (60%)]\tLoss: 0.357038\n",
      "Train Epoch: 2 [4480/6381 (70%)]\tLoss: 0.040710\n",
      "Train Epoch: 2 [5120/6381 (80%)]\tLoss: 0.139810\n",
      "Train Epoch: 2 [5760/6381 (90%)]\tLoss: 0.032341\n",
      "\n",
      "Accuracy: 1085/1119 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/6381 (0%)]\tLoss: 0.024230\n",
      "Train Epoch: 3 [640/6381 (10%)]\tLoss: 0.154390\n",
      "Train Epoch: 3 [1280/6381 (20%)]\tLoss: 0.089951\n",
      "Train Epoch: 3 [1920/6381 (30%)]\tLoss: 0.051746\n",
      "Train Epoch: 3 [2560/6381 (40%)]\tLoss: 0.072450\n",
      "Train Epoch: 3 [3200/6381 (50%)]\tLoss: 0.050164\n",
      "Train Epoch: 3 [3840/6381 (60%)]\tLoss: 0.072578\n",
      "Train Epoch: 3 [4480/6381 (70%)]\tLoss: 0.153464\n",
      "Train Epoch: 3 [5120/6381 (80%)]\tLoss: 0.046233\n",
      "Train Epoch: 3 [5760/6381 (90%)]\tLoss: 0.105697\n",
      "\n",
      "Accuracy: 1094/1119 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/6381 (0%)]\tLoss: 0.117011\n",
      "Train Epoch: 4 [640/6381 (10%)]\tLoss: 0.068497\n",
      "Train Epoch: 4 [1280/6381 (20%)]\tLoss: 0.021972\n",
      "Train Epoch: 4 [1920/6381 (30%)]\tLoss: 0.028684\n",
      "Train Epoch: 4 [2560/6381 (40%)]\tLoss: 0.055447\n",
      "Train Epoch: 4 [3200/6381 (50%)]\tLoss: 0.037287\n",
      "Train Epoch: 4 [3840/6381 (60%)]\tLoss: 0.034466\n",
      "Train Epoch: 4 [4480/6381 (70%)]\tLoss: 0.005698\n",
      "Train Epoch: 4 [5120/6381 (80%)]\tLoss: 0.056915\n",
      "Train Epoch: 4 [5760/6381 (90%)]\tLoss: 0.041418\n",
      "\n",
      "Accuracy: 1091/1119 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/6381 (0%)]\tLoss: 0.046746\n",
      "Train Epoch: 5 [640/6381 (10%)]\tLoss: 0.008816\n",
      "Train Epoch: 5 [1280/6381 (20%)]\tLoss: 0.076923\n",
      "Train Epoch: 5 [1920/6381 (30%)]\tLoss: 0.077487\n",
      "Train Epoch: 5 [2560/6381 (40%)]\tLoss: 0.070123\n",
      "Train Epoch: 5 [3200/6381 (50%)]\tLoss: 0.045536\n",
      "Train Epoch: 5 [3840/6381 (60%)]\tLoss: 0.115318\n",
      "Train Epoch: 5 [4480/6381 (70%)]\tLoss: 0.014813\n",
      "Train Epoch: 5 [5120/6381 (80%)]\tLoss: 0.159959\n",
      "Train Epoch: 5 [5760/6381 (90%)]\tLoss: 0.037805\n",
      "\n",
      "Accuracy: 1097/1119 (98%)\n",
      "\n",
      "Training Set (0.125 Subset):\n",
      "\n",
      "Accuracy: 6353/6381 (100%)\n",
      "\n",
      "Test Set (0.125 Subset):\n",
      "\n",
      "Accuracy: 9807/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/12754 (0%)]\tLoss: 2.307036\n",
      "Train Epoch: 1 [640/12754 (5%)]\tLoss: 2.259247\n",
      "Train Epoch: 1 [1280/12754 (10%)]\tLoss: 0.796350\n",
      "Train Epoch: 1 [1920/12754 (15%)]\tLoss: 0.392383\n",
      "Train Epoch: 1 [2560/12754 (20%)]\tLoss: 0.218966\n",
      "Train Epoch: 1 [3200/12754 (25%)]\tLoss: 0.304692\n",
      "Train Epoch: 1 [3840/12754 (30%)]\tLoss: 0.301673\n",
      "Train Epoch: 1 [4480/12754 (35%)]\tLoss: 0.236398\n",
      "Train Epoch: 1 [5120/12754 (40%)]\tLoss: 0.326393\n",
      "Train Epoch: 1 [5760/12754 (45%)]\tLoss: 0.178649\n",
      "Train Epoch: 1 [6400/12754 (50%)]\tLoss: 0.221239\n",
      "Train Epoch: 1 [7040/12754 (55%)]\tLoss: 0.078719\n",
      "Train Epoch: 1 [7680/12754 (60%)]\tLoss: 0.220867\n",
      "Train Epoch: 1 [8320/12754 (65%)]\tLoss: 0.190671\n",
      "Train Epoch: 1 [8960/12754 (70%)]\tLoss: 0.229216\n",
      "Train Epoch: 1 [9600/12754 (75%)]\tLoss: 0.476597\n",
      "Train Epoch: 1 [10240/12754 (80%)]\tLoss: 0.224046\n",
      "Train Epoch: 1 [10880/12754 (85%)]\tLoss: 0.069163\n",
      "Train Epoch: 1 [11520/12754 (90%)]\tLoss: 0.100306\n",
      "Train Epoch: 1 [12160/12754 (95%)]\tLoss: 0.046526\n",
      "\n",
      "Accuracy: 2072/2246 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/12754 (0%)]\tLoss: 0.194427\n",
      "Train Epoch: 2 [640/12754 (5%)]\tLoss: 0.121901\n",
      "Train Epoch: 2 [1280/12754 (10%)]\tLoss: 0.084352\n",
      "Train Epoch: 2 [1920/12754 (15%)]\tLoss: 0.232306\n",
      "Train Epoch: 2 [2560/12754 (20%)]\tLoss: 0.085341\n",
      "Train Epoch: 2 [3200/12754 (25%)]\tLoss: 0.100693\n",
      "Train Epoch: 2 [3840/12754 (30%)]\tLoss: 0.202753\n",
      "Train Epoch: 2 [4480/12754 (35%)]\tLoss: 0.022212\n",
      "Train Epoch: 2 [5120/12754 (40%)]\tLoss: 0.039534\n",
      "Train Epoch: 2 [5760/12754 (45%)]\tLoss: 0.202011\n",
      "Train Epoch: 2 [6400/12754 (50%)]\tLoss: 0.062023\n",
      "Train Epoch: 2 [7040/12754 (55%)]\tLoss: 0.080270\n",
      "Train Epoch: 2 [7680/12754 (60%)]\tLoss: 0.113399\n",
      "Train Epoch: 2 [8320/12754 (65%)]\tLoss: 0.110696\n",
      "Train Epoch: 2 [8960/12754 (70%)]\tLoss: 0.180429\n",
      "Train Epoch: 2 [9600/12754 (75%)]\tLoss: 0.145325\n",
      "Train Epoch: 2 [10240/12754 (80%)]\tLoss: 0.084060\n",
      "Train Epoch: 2 [10880/12754 (85%)]\tLoss: 0.072313\n",
      "Train Epoch: 2 [11520/12754 (90%)]\tLoss: 0.046061\n",
      "Train Epoch: 2 [12160/12754 (95%)]\tLoss: 0.225536\n",
      "\n",
      "Accuracy: 2205/2246 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/12754 (0%)]\tLoss: 0.102175\n",
      "Train Epoch: 3 [640/12754 (5%)]\tLoss: 0.041459\n",
      "Train Epoch: 3 [1280/12754 (10%)]\tLoss: 0.009715\n",
      "Train Epoch: 3 [1920/12754 (15%)]\tLoss: 0.118389\n",
      "Train Epoch: 3 [2560/12754 (20%)]\tLoss: 0.104893\n",
      "Train Epoch: 3 [3200/12754 (25%)]\tLoss: 0.073520\n",
      "Train Epoch: 3 [3840/12754 (30%)]\tLoss: 0.013858\n",
      "Train Epoch: 3 [4480/12754 (35%)]\tLoss: 0.031073\n",
      "Train Epoch: 3 [5120/12754 (40%)]\tLoss: 0.004399\n",
      "Train Epoch: 3 [5760/12754 (45%)]\tLoss: 0.025681\n",
      "Train Epoch: 3 [6400/12754 (50%)]\tLoss: 0.019086\n",
      "Train Epoch: 3 [7040/12754 (55%)]\tLoss: 0.039407\n",
      "Train Epoch: 3 [7680/12754 (60%)]\tLoss: 0.027041\n",
      "Train Epoch: 3 [8320/12754 (65%)]\tLoss: 0.070681\n",
      "Train Epoch: 3 [8960/12754 (70%)]\tLoss: 0.034492\n",
      "Train Epoch: 3 [9600/12754 (75%)]\tLoss: 0.046690\n",
      "Train Epoch: 3 [10240/12754 (80%)]\tLoss: 0.008511\n",
      "Train Epoch: 3 [10880/12754 (85%)]\tLoss: 0.014328\n",
      "Train Epoch: 3 [11520/12754 (90%)]\tLoss: 0.019862\n",
      "Train Epoch: 3 [12160/12754 (95%)]\tLoss: 0.029680\n",
      "\n",
      "Accuracy: 2206/2246 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/12754 (0%)]\tLoss: 0.029394\n",
      "Train Epoch: 4 [640/12754 (5%)]\tLoss: 0.046452\n",
      "Train Epoch: 4 [1280/12754 (10%)]\tLoss: 0.123825\n",
      "Train Epoch: 4 [1920/12754 (15%)]\tLoss: 0.058251\n",
      "Train Epoch: 4 [2560/12754 (20%)]\tLoss: 0.018731\n",
      "Train Epoch: 4 [3200/12754 (25%)]\tLoss: 0.049368\n",
      "Train Epoch: 4 [3840/12754 (30%)]\tLoss: 0.033946\n",
      "Train Epoch: 4 [4480/12754 (35%)]\tLoss: 0.016495\n",
      "Train Epoch: 4 [5120/12754 (40%)]\tLoss: 0.080534\n",
      "Train Epoch: 4 [5760/12754 (45%)]\tLoss: 0.006907\n",
      "Train Epoch: 4 [6400/12754 (50%)]\tLoss: 0.007791\n",
      "Train Epoch: 4 [7040/12754 (55%)]\tLoss: 0.010416\n",
      "Train Epoch: 4 [7680/12754 (60%)]\tLoss: 0.020723\n",
      "Train Epoch: 4 [8320/12754 (65%)]\tLoss: 0.041759\n",
      "Train Epoch: 4 [8960/12754 (70%)]\tLoss: 0.017341\n",
      "Train Epoch: 4 [9600/12754 (75%)]\tLoss: 0.004572\n",
      "Train Epoch: 4 [10240/12754 (80%)]\tLoss: 0.001573\n",
      "Train Epoch: 4 [10880/12754 (85%)]\tLoss: 0.020551\n",
      "Train Epoch: 4 [11520/12754 (90%)]\tLoss: 0.008824\n",
      "Train Epoch: 4 [12160/12754 (95%)]\tLoss: 0.046116\n",
      "\n",
      "Accuracy: 2214/2246 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/12754 (0%)]\tLoss: 0.008119\n",
      "Train Epoch: 5 [640/12754 (5%)]\tLoss: 0.042404\n",
      "Train Epoch: 5 [1280/12754 (10%)]\tLoss: 0.016720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [1920/12754 (15%)]\tLoss: 0.042932\n",
      "Train Epoch: 5 [2560/12754 (20%)]\tLoss: 0.008047\n",
      "Train Epoch: 5 [3200/12754 (25%)]\tLoss: 0.073221\n",
      "Train Epoch: 5 [3840/12754 (30%)]\tLoss: 0.007362\n",
      "Train Epoch: 5 [4480/12754 (35%)]\tLoss: 0.010274\n",
      "Train Epoch: 5 [5120/12754 (40%)]\tLoss: 0.003046\n",
      "Train Epoch: 5 [5760/12754 (45%)]\tLoss: 0.007613\n",
      "Train Epoch: 5 [6400/12754 (50%)]\tLoss: 0.002908\n",
      "Train Epoch: 5 [7040/12754 (55%)]\tLoss: 0.018687\n",
      "Train Epoch: 5 [7680/12754 (60%)]\tLoss: 0.008861\n",
      "Train Epoch: 5 [8320/12754 (65%)]\tLoss: 0.009768\n",
      "Train Epoch: 5 [8960/12754 (70%)]\tLoss: 0.003517\n",
      "Train Epoch: 5 [9600/12754 (75%)]\tLoss: 0.027097\n",
      "Train Epoch: 5 [10240/12754 (80%)]\tLoss: 0.017046\n",
      "Train Epoch: 5 [10880/12754 (85%)]\tLoss: 0.028635\n",
      "Train Epoch: 5 [11520/12754 (90%)]\tLoss: 0.002286\n",
      "Train Epoch: 5 [12160/12754 (95%)]\tLoss: 0.009921\n",
      "\n",
      "Accuracy: 2212/2246 (98%)\n",
      "\n",
      "Training Set (0.25 Subset):\n",
      "\n",
      "Accuracy: 12704/12754 (100%)\n",
      "\n",
      "Test Set (0.25 Subset):\n",
      "\n",
      "Accuracy: 9842/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/25505 (0%)]\tLoss: 2.338892\n",
      "Train Epoch: 1 [640/25505 (3%)]\tLoss: 1.285711\n",
      "Train Epoch: 1 [1280/25505 (5%)]\tLoss: 0.827060\n",
      "Train Epoch: 1 [1920/25505 (8%)]\tLoss: 0.442648\n",
      "Train Epoch: 1 [2560/25505 (10%)]\tLoss: 0.318864\n",
      "Train Epoch: 1 [3200/25505 (13%)]\tLoss: 0.265375\n",
      "Train Epoch: 1 [3840/25505 (15%)]\tLoss: 0.162553\n",
      "Train Epoch: 1 [4480/25505 (18%)]\tLoss: 0.099103\n",
      "Train Epoch: 1 [5120/25505 (20%)]\tLoss: 0.338026\n",
      "Train Epoch: 1 [5760/25505 (23%)]\tLoss: 0.073845\n",
      "Train Epoch: 1 [6400/25505 (25%)]\tLoss: 0.231972\n",
      "Train Epoch: 1 [7040/25505 (28%)]\tLoss: 0.214245\n",
      "Train Epoch: 1 [7680/25505 (30%)]\tLoss: 0.067185\n",
      "Train Epoch: 1 [8320/25505 (33%)]\tLoss: 0.145757\n",
      "Train Epoch: 1 [8960/25505 (35%)]\tLoss: 0.096656\n",
      "Train Epoch: 1 [9600/25505 (38%)]\tLoss: 0.282415\n",
      "Train Epoch: 1 [10240/25505 (40%)]\tLoss: 0.197071\n",
      "Train Epoch: 1 [10880/25505 (43%)]\tLoss: 0.183607\n",
      "Train Epoch: 1 [11520/25505 (45%)]\tLoss: 0.071462\n",
      "Train Epoch: 1 [12160/25505 (48%)]\tLoss: 0.229020\n",
      "Train Epoch: 1 [12800/25505 (50%)]\tLoss: 0.101775\n",
      "Train Epoch: 1 [13440/25505 (53%)]\tLoss: 0.110020\n",
      "Train Epoch: 1 [14080/25505 (55%)]\tLoss: 0.254808\n",
      "Train Epoch: 1 [14720/25505 (58%)]\tLoss: 0.055635\n",
      "Train Epoch: 1 [15360/25505 (60%)]\tLoss: 0.224315\n",
      "Train Epoch: 1 [16000/25505 (63%)]\tLoss: 0.045523\n",
      "Train Epoch: 1 [16640/25505 (65%)]\tLoss: 0.026900\n",
      "Train Epoch: 1 [17280/25505 (68%)]\tLoss: 0.069532\n",
      "Train Epoch: 1 [17920/25505 (70%)]\tLoss: 0.150614\n",
      "Train Epoch: 1 [18560/25505 (73%)]\tLoss: 0.201905\n",
      "Train Epoch: 1 [19200/25505 (75%)]\tLoss: 0.066288\n",
      "Train Epoch: 1 [19840/25505 (78%)]\tLoss: 0.024992\n",
      "Train Epoch: 1 [20480/25505 (80%)]\tLoss: 0.105756\n",
      "Train Epoch: 1 [21120/25505 (83%)]\tLoss: 0.079173\n",
      "Train Epoch: 1 [21760/25505 (85%)]\tLoss: 0.232989\n",
      "Train Epoch: 1 [22400/25505 (88%)]\tLoss: 0.117646\n",
      "Train Epoch: 1 [23040/25505 (90%)]\tLoss: 0.026365\n",
      "Train Epoch: 1 [23680/25505 (93%)]\tLoss: 0.096561\n",
      "Train Epoch: 1 [24320/25505 (95%)]\tLoss: 0.071129\n",
      "Train Epoch: 1 [24960/25505 (98%)]\tLoss: 0.043882\n",
      "\n",
      "Accuracy: 4402/4495 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/25505 (0%)]\tLoss: 0.020985\n",
      "Train Epoch: 2 [640/25505 (3%)]\tLoss: 0.023694\n",
      "Train Epoch: 2 [1280/25505 (5%)]\tLoss: 0.112680\n",
      "Train Epoch: 2 [1920/25505 (8%)]\tLoss: 0.041389\n",
      "Train Epoch: 2 [2560/25505 (10%)]\tLoss: 0.077449\n",
      "Train Epoch: 2 [3200/25505 (13%)]\tLoss: 0.069368\n",
      "Train Epoch: 2 [3840/25505 (15%)]\tLoss: 0.134216\n",
      "Train Epoch: 2 [4480/25505 (18%)]\tLoss: 0.037226\n",
      "Train Epoch: 2 [5120/25505 (20%)]\tLoss: 0.020364\n",
      "Train Epoch: 2 [5760/25505 (23%)]\tLoss: 0.037762\n",
      "Train Epoch: 2 [6400/25505 (25%)]\tLoss: 0.022747\n",
      "Train Epoch: 2 [7040/25505 (28%)]\tLoss: 0.065166\n",
      "Train Epoch: 2 [7680/25505 (30%)]\tLoss: 0.087489\n",
      "Train Epoch: 2 [8320/25505 (33%)]\tLoss: 0.130756\n",
      "Train Epoch: 2 [8960/25505 (35%)]\tLoss: 0.008912\n",
      "Train Epoch: 2 [9600/25505 (38%)]\tLoss: 0.074643\n",
      "Train Epoch: 2 [10240/25505 (40%)]\tLoss: 0.020421\n",
      "Train Epoch: 2 [10880/25505 (43%)]\tLoss: 0.078856\n",
      "Train Epoch: 2 [11520/25505 (45%)]\tLoss: 0.059357\n",
      "Train Epoch: 2 [12160/25505 (48%)]\tLoss: 0.131570\n",
      "Train Epoch: 2 [12800/25505 (50%)]\tLoss: 0.148520\n",
      "Train Epoch: 2 [13440/25505 (53%)]\tLoss: 0.059386\n",
      "Train Epoch: 2 [14080/25505 (55%)]\tLoss: 0.032670\n",
      "Train Epoch: 2 [14720/25505 (58%)]\tLoss: 0.050903\n",
      "Train Epoch: 2 [15360/25505 (60%)]\tLoss: 0.096439\n",
      "Train Epoch: 2 [16000/25505 (63%)]\tLoss: 0.018619\n",
      "Train Epoch: 2 [16640/25505 (65%)]\tLoss: 0.008988\n",
      "Train Epoch: 2 [17280/25505 (68%)]\tLoss: 0.023223\n",
      "Train Epoch: 2 [17920/25505 (70%)]\tLoss: 0.037758\n",
      "Train Epoch: 2 [18560/25505 (73%)]\tLoss: 0.058444\n",
      "Train Epoch: 2 [19200/25505 (75%)]\tLoss: 0.068068\n",
      "Train Epoch: 2 [19840/25505 (78%)]\tLoss: 0.057779\n",
      "Train Epoch: 2 [20480/25505 (80%)]\tLoss: 0.014952\n",
      "Train Epoch: 2 [21120/25505 (83%)]\tLoss: 0.020882\n",
      "Train Epoch: 2 [21760/25505 (85%)]\tLoss: 0.046209\n",
      "Train Epoch: 2 [22400/25505 (88%)]\tLoss: 0.005391\n",
      "Train Epoch: 2 [23040/25505 (90%)]\tLoss: 0.150149\n",
      "Train Epoch: 2 [23680/25505 (93%)]\tLoss: 0.034636\n",
      "Train Epoch: 2 [24320/25505 (95%)]\tLoss: 0.165414\n",
      "Train Epoch: 2 [24960/25505 (98%)]\tLoss: 0.014079\n",
      "\n",
      "Accuracy: 4429/4495 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/25505 (0%)]\tLoss: 0.008554\n",
      "Train Epoch: 3 [640/25505 (3%)]\tLoss: 0.006422\n",
      "Train Epoch: 3 [1280/25505 (5%)]\tLoss: 0.015815\n",
      "Train Epoch: 3 [1920/25505 (8%)]\tLoss: 0.089120\n",
      "Train Epoch: 3 [2560/25505 (10%)]\tLoss: 0.109444\n",
      "Train Epoch: 3 [3200/25505 (13%)]\tLoss: 0.022283\n",
      "Train Epoch: 3 [3840/25505 (15%)]\tLoss: 0.016055\n",
      "Train Epoch: 3 [4480/25505 (18%)]\tLoss: 0.023107\n",
      "Train Epoch: 3 [5120/25505 (20%)]\tLoss: 0.112324\n",
      "Train Epoch: 3 [5760/25505 (23%)]\tLoss: 0.018457\n",
      "Train Epoch: 3 [6400/25505 (25%)]\tLoss: 0.073236\n",
      "Train Epoch: 3 [7040/25505 (28%)]\tLoss: 0.173515\n",
      "Train Epoch: 3 [7680/25505 (30%)]\tLoss: 0.022258\n",
      "Train Epoch: 3 [8320/25505 (33%)]\tLoss: 0.060848\n",
      "Train Epoch: 3 [8960/25505 (35%)]\tLoss: 0.014839\n",
      "Train Epoch: 3 [9600/25505 (38%)]\tLoss: 0.042156\n",
      "Train Epoch: 3 [10240/25505 (40%)]\tLoss: 0.009401\n",
      "Train Epoch: 3 [10880/25505 (43%)]\tLoss: 0.017634\n",
      "Train Epoch: 3 [11520/25505 (45%)]\tLoss: 0.035735\n",
      "Train Epoch: 3 [12160/25505 (48%)]\tLoss: 0.011338\n",
      "Train Epoch: 3 [12800/25505 (50%)]\tLoss: 0.047608\n",
      "Train Epoch: 3 [13440/25505 (53%)]\tLoss: 0.072312\n",
      "Train Epoch: 3 [14080/25505 (55%)]\tLoss: 0.005167\n",
      "Train Epoch: 3 [14720/25505 (58%)]\tLoss: 0.000507\n",
      "Train Epoch: 3 [15360/25505 (60%)]\tLoss: 0.045207\n",
      "Train Epoch: 3 [16000/25505 (63%)]\tLoss: 0.028089\n",
      "Train Epoch: 3 [16640/25505 (65%)]\tLoss: 0.000738\n",
      "Train Epoch: 3 [17280/25505 (68%)]\tLoss: 0.030029\n",
      "Train Epoch: 3 [17920/25505 (70%)]\tLoss: 0.023462\n",
      "Train Epoch: 3 [18560/25505 (73%)]\tLoss: 0.026104\n",
      "Train Epoch: 3 [19200/25505 (75%)]\tLoss: 0.080498\n",
      "Train Epoch: 3 [19840/25505 (78%)]\tLoss: 0.012290\n",
      "Train Epoch: 3 [20480/25505 (80%)]\tLoss: 0.003937\n",
      "Train Epoch: 3 [21120/25505 (83%)]\tLoss: 0.017277\n",
      "Train Epoch: 3 [21760/25505 (85%)]\tLoss: 0.228614\n",
      "Train Epoch: 3 [22400/25505 (88%)]\tLoss: 0.123351\n",
      "Train Epoch: 3 [23040/25505 (90%)]\tLoss: 0.057917\n",
      "Train Epoch: 3 [23680/25505 (93%)]\tLoss: 0.037734\n",
      "Train Epoch: 3 [24320/25505 (95%)]\tLoss: 0.013258\n",
      "Train Epoch: 3 [24960/25505 (98%)]\tLoss: 0.006838\n",
      "\n",
      "Accuracy: 4434/4495 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/25505 (0%)]\tLoss: 0.011029\n",
      "Train Epoch: 4 [640/25505 (3%)]\tLoss: 0.053025\n",
      "Train Epoch: 4 [1280/25505 (5%)]\tLoss: 0.001873\n",
      "Train Epoch: 4 [1920/25505 (8%)]\tLoss: 0.028622\n",
      "Train Epoch: 4 [2560/25505 (10%)]\tLoss: 0.047753\n",
      "Train Epoch: 4 [3200/25505 (13%)]\tLoss: 0.001978\n",
      "Train Epoch: 4 [3840/25505 (15%)]\tLoss: 0.001199\n",
      "Train Epoch: 4 [4480/25505 (18%)]\tLoss: 0.011884\n",
      "Train Epoch: 4 [5120/25505 (20%)]\tLoss: 0.023345\n",
      "Train Epoch: 4 [5760/25505 (23%)]\tLoss: 0.010781\n",
      "Train Epoch: 4 [6400/25505 (25%)]\tLoss: 0.004664\n",
      "Train Epoch: 4 [7040/25505 (28%)]\tLoss: 0.013602\n",
      "Train Epoch: 4 [7680/25505 (30%)]\tLoss: 0.001475\n",
      "Train Epoch: 4 [8320/25505 (33%)]\tLoss: 0.056389\n",
      "Train Epoch: 4 [8960/25505 (35%)]\tLoss: 0.037163\n",
      "Train Epoch: 4 [9600/25505 (38%)]\tLoss: 0.037994\n",
      "Train Epoch: 4 [10240/25505 (40%)]\tLoss: 0.007129\n",
      "Train Epoch: 4 [10880/25505 (43%)]\tLoss: 0.009860\n",
      "Train Epoch: 4 [11520/25505 (45%)]\tLoss: 0.036571\n",
      "Train Epoch: 4 [12160/25505 (48%)]\tLoss: 0.023168\n",
      "Train Epoch: 4 [12800/25505 (50%)]\tLoss: 0.007215\n",
      "Train Epoch: 4 [13440/25505 (53%)]\tLoss: 0.005740\n",
      "Train Epoch: 4 [14080/25505 (55%)]\tLoss: 0.003337\n",
      "Train Epoch: 4 [14720/25505 (58%)]\tLoss: 0.008163\n",
      "Train Epoch: 4 [15360/25505 (60%)]\tLoss: 0.010577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [16000/25505 (63%)]\tLoss: 0.000687\n",
      "Train Epoch: 4 [16640/25505 (65%)]\tLoss: 0.000708\n",
      "Train Epoch: 4 [17280/25505 (68%)]\tLoss: 0.004551\n",
      "Train Epoch: 4 [17920/25505 (70%)]\tLoss: 0.001925\n",
      "Train Epoch: 4 [18560/25505 (73%)]\tLoss: 0.005476\n",
      "Train Epoch: 4 [19200/25505 (75%)]\tLoss: 0.000860\n",
      "Train Epoch: 4 [19840/25505 (78%)]\tLoss: 0.000604\n",
      "Train Epoch: 4 [20480/25505 (80%)]\tLoss: 0.051644\n",
      "Train Epoch: 4 [21120/25505 (83%)]\tLoss: 0.085564\n",
      "Train Epoch: 4 [21760/25505 (85%)]\tLoss: 0.000527\n",
      "Train Epoch: 4 [22400/25505 (88%)]\tLoss: 0.011759\n",
      "Train Epoch: 4 [23040/25505 (90%)]\tLoss: 0.037251\n",
      "Train Epoch: 4 [23680/25505 (93%)]\tLoss: 0.008252\n",
      "Train Epoch: 4 [24320/25505 (95%)]\tLoss: 0.009127\n",
      "Train Epoch: 4 [24960/25505 (98%)]\tLoss: 0.138204\n",
      "\n",
      "Accuracy: 4441/4495 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/25505 (0%)]\tLoss: 0.005307\n",
      "Train Epoch: 5 [640/25505 (3%)]\tLoss: 0.000783\n",
      "Train Epoch: 5 [1280/25505 (5%)]\tLoss: 0.004699\n",
      "Train Epoch: 5 [1920/25505 (8%)]\tLoss: 0.014400\n",
      "Train Epoch: 5 [2560/25505 (10%)]\tLoss: 0.020835\n",
      "Train Epoch: 5 [3200/25505 (13%)]\tLoss: 0.003765\n",
      "Train Epoch: 5 [3840/25505 (15%)]\tLoss: 0.003083\n",
      "Train Epoch: 5 [4480/25505 (18%)]\tLoss: 0.002602\n",
      "Train Epoch: 5 [5120/25505 (20%)]\tLoss: 0.017870\n",
      "Train Epoch: 5 [5760/25505 (23%)]\tLoss: 0.003077\n",
      "Train Epoch: 5 [6400/25505 (25%)]\tLoss: 0.009725\n",
      "Train Epoch: 5 [7040/25505 (28%)]\tLoss: 0.025267\n",
      "Train Epoch: 5 [7680/25505 (30%)]\tLoss: 0.000701\n",
      "Train Epoch: 5 [8320/25505 (33%)]\tLoss: 0.021419\n",
      "Train Epoch: 5 [8960/25505 (35%)]\tLoss: 0.004142\n",
      "Train Epoch: 5 [9600/25505 (38%)]\tLoss: 0.024049\n",
      "Train Epoch: 5 [10240/25505 (40%)]\tLoss: 0.004316\n",
      "Train Epoch: 5 [10880/25505 (43%)]\tLoss: 0.005312\n",
      "Train Epoch: 5 [11520/25505 (45%)]\tLoss: 0.000472\n",
      "Train Epoch: 5 [12160/25505 (48%)]\tLoss: 0.002871\n",
      "Train Epoch: 5 [12800/25505 (50%)]\tLoss: 0.001998\n",
      "Train Epoch: 5 [13440/25505 (53%)]\tLoss: 0.000994\n",
      "Train Epoch: 5 [14080/25505 (55%)]\tLoss: 0.010656\n",
      "Train Epoch: 5 [14720/25505 (58%)]\tLoss: 0.082737\n",
      "Train Epoch: 5 [15360/25505 (60%)]\tLoss: 0.014224\n",
      "Train Epoch: 5 [16000/25505 (63%)]\tLoss: 0.011148\n",
      "Train Epoch: 5 [16640/25505 (65%)]\tLoss: 0.012312\n",
      "Train Epoch: 5 [17280/25505 (68%)]\tLoss: 0.018561\n",
      "Train Epoch: 5 [17920/25505 (70%)]\tLoss: 0.001836\n",
      "Train Epoch: 5 [18560/25505 (73%)]\tLoss: 0.020404\n",
      "Train Epoch: 5 [19200/25505 (75%)]\tLoss: 0.005813\n",
      "Train Epoch: 5 [19840/25505 (78%)]\tLoss: 0.035375\n",
      "Train Epoch: 5 [20480/25505 (80%)]\tLoss: 0.086371\n",
      "Train Epoch: 5 [21120/25505 (83%)]\tLoss: 0.002768\n",
      "Train Epoch: 5 [21760/25505 (85%)]\tLoss: 0.000623\n",
      "Train Epoch: 5 [22400/25505 (88%)]\tLoss: 0.004591\n",
      "Train Epoch: 5 [23040/25505 (90%)]\tLoss: 0.010441\n",
      "Train Epoch: 5 [23680/25505 (93%)]\tLoss: 0.021195\n",
      "Train Epoch: 5 [24320/25505 (95%)]\tLoss: 0.063689\n",
      "Train Epoch: 5 [24960/25505 (98%)]\tLoss: 0.003783\n",
      "\n",
      "Accuracy: 4446/4495 (99%)\n",
      "\n",
      "Training Set (0.5 Subset):\n",
      "\n",
      "Accuracy: 25445/25505 (100%)\n",
      "\n",
      "Test Set (0.5 Subset):\n",
      "\n",
      "Accuracy: 9887/10000 (99%)\n",
      "\n",
      "Train Epoch: 1 [0/51005 (0%)]\tLoss: 2.336793\n",
      "Train Epoch: 1 [640/51005 (1%)]\tLoss: 1.827948\n",
      "Train Epoch: 1 [1280/51005 (3%)]\tLoss: 0.747410\n",
      "Train Epoch: 1 [1920/51005 (4%)]\tLoss: 0.421626\n",
      "Train Epoch: 1 [2560/51005 (5%)]\tLoss: 0.874417\n",
      "Train Epoch: 1 [3200/51005 (6%)]\tLoss: 0.177436\n",
      "Train Epoch: 1 [3840/51005 (8%)]\tLoss: 0.409591\n",
      "Train Epoch: 1 [4480/51005 (9%)]\tLoss: 0.362214\n",
      "Train Epoch: 1 [5120/51005 (10%)]\tLoss: 0.371659\n",
      "Train Epoch: 1 [5760/51005 (11%)]\tLoss: 0.293733\n",
      "Train Epoch: 1 [6400/51005 (13%)]\tLoss: 0.080080\n",
      "Train Epoch: 1 [7040/51005 (14%)]\tLoss: 0.362958\n",
      "Train Epoch: 1 [7680/51005 (15%)]\tLoss: 0.187496\n",
      "Train Epoch: 1 [8320/51005 (16%)]\tLoss: 0.082955\n",
      "Train Epoch: 1 [8960/51005 (18%)]\tLoss: 0.065455\n",
      "Train Epoch: 1 [9600/51005 (19%)]\tLoss: 0.143855\n",
      "Train Epoch: 1 [10240/51005 (20%)]\tLoss: 0.097887\n",
      "Train Epoch: 1 [10880/51005 (21%)]\tLoss: 0.092236\n",
      "Train Epoch: 1 [11520/51005 (23%)]\tLoss: 0.192765\n",
      "Train Epoch: 1 [12160/51005 (24%)]\tLoss: 0.240044\n",
      "Train Epoch: 1 [12800/51005 (25%)]\tLoss: 0.105982\n",
      "Train Epoch: 1 [13440/51005 (26%)]\tLoss: 0.118588\n",
      "Train Epoch: 1 [14080/51005 (28%)]\tLoss: 0.290701\n",
      "Train Epoch: 1 [14720/51005 (29%)]\tLoss: 0.182288\n",
      "Train Epoch: 1 [15360/51005 (30%)]\tLoss: 0.036038\n",
      "Train Epoch: 1 [16000/51005 (31%)]\tLoss: 0.191744\n",
      "Train Epoch: 1 [16640/51005 (33%)]\tLoss: 0.131867\n",
      "Train Epoch: 1 [17280/51005 (34%)]\tLoss: 0.190969\n",
      "Train Epoch: 1 [17920/51005 (35%)]\tLoss: 0.052506\n",
      "Train Epoch: 1 [18560/51005 (36%)]\tLoss: 0.106911\n",
      "Train Epoch: 1 [19200/51005 (38%)]\tLoss: 0.088023\n",
      "Train Epoch: 1 [19840/51005 (39%)]\tLoss: 0.123385\n",
      "Train Epoch: 1 [20480/51005 (40%)]\tLoss: 0.081560\n",
      "Train Epoch: 1 [21120/51005 (41%)]\tLoss: 0.115042\n",
      "Train Epoch: 1 [21760/51005 (43%)]\tLoss: 0.040945\n",
      "Train Epoch: 1 [22400/51005 (44%)]\tLoss: 0.187114\n",
      "Train Epoch: 1 [23040/51005 (45%)]\tLoss: 0.030559\n",
      "Train Epoch: 1 [23680/51005 (46%)]\tLoss: 0.064264\n",
      "Train Epoch: 1 [24320/51005 (48%)]\tLoss: 0.087492\n",
      "Train Epoch: 1 [24960/51005 (49%)]\tLoss: 0.031442\n",
      "Train Epoch: 1 [25600/51005 (50%)]\tLoss: 0.093576\n",
      "Train Epoch: 1 [26240/51005 (51%)]\tLoss: 0.052025\n",
      "Train Epoch: 1 [26880/51005 (53%)]\tLoss: 0.020703\n",
      "Train Epoch: 1 [27520/51005 (54%)]\tLoss: 0.009848\n",
      "Train Epoch: 1 [28160/51005 (55%)]\tLoss: 0.115637\n",
      "Train Epoch: 1 [28800/51005 (56%)]\tLoss: 0.203084\n",
      "Train Epoch: 1 [29440/51005 (58%)]\tLoss: 0.057468\n",
      "Train Epoch: 1 [30080/51005 (59%)]\tLoss: 0.040040\n",
      "Train Epoch: 1 [30720/51005 (60%)]\tLoss: 0.065910\n",
      "Train Epoch: 1 [31360/51005 (61%)]\tLoss: 0.130611\n",
      "Train Epoch: 1 [32000/51005 (63%)]\tLoss: 0.139956\n",
      "Train Epoch: 1 [32640/51005 (64%)]\tLoss: 0.008501\n",
      "Train Epoch: 1 [33280/51005 (65%)]\tLoss: 0.035402\n",
      "Train Epoch: 1 [33920/51005 (66%)]\tLoss: 0.062907\n",
      "Train Epoch: 1 [34560/51005 (68%)]\tLoss: 0.127719\n",
      "Train Epoch: 1 [35200/51005 (69%)]\tLoss: 0.078137\n",
      "Train Epoch: 1 [35840/51005 (70%)]\tLoss: 0.017557\n",
      "Train Epoch: 1 [36480/51005 (72%)]\tLoss: 0.055118\n",
      "Train Epoch: 1 [37120/51005 (73%)]\tLoss: 0.025909\n",
      "Train Epoch: 1 [37760/51005 (74%)]\tLoss: 0.234883\n",
      "Train Epoch: 1 [38400/51005 (75%)]\tLoss: 0.032448\n",
      "Train Epoch: 1 [39040/51005 (77%)]\tLoss: 0.021839\n",
      "Train Epoch: 1 [39680/51005 (78%)]\tLoss: 0.010501\n",
      "Train Epoch: 1 [40320/51005 (79%)]\tLoss: 0.072541\n",
      "Train Epoch: 1 [40960/51005 (80%)]\tLoss: 0.065548\n",
      "Train Epoch: 1 [41600/51005 (82%)]\tLoss: 0.171710\n",
      "Train Epoch: 1 [42240/51005 (83%)]\tLoss: 0.177733\n",
      "Train Epoch: 1 [42880/51005 (84%)]\tLoss: 0.054402\n",
      "Train Epoch: 1 [43520/51005 (85%)]\tLoss: 0.114512\n",
      "Train Epoch: 1 [44160/51005 (87%)]\tLoss: 0.139911\n",
      "Train Epoch: 1 [44800/51005 (88%)]\tLoss: 0.126614\n",
      "Train Epoch: 1 [45440/51005 (89%)]\tLoss: 0.078382\n",
      "Train Epoch: 1 [46080/51005 (90%)]\tLoss: 0.105269\n",
      "Train Epoch: 1 [46720/51005 (92%)]\tLoss: 0.125704\n",
      "Train Epoch: 1 [47360/51005 (93%)]\tLoss: 0.187520\n",
      "Train Epoch: 1 [48000/51005 (94%)]\tLoss: 0.004613\n",
      "Train Epoch: 1 [48640/51005 (95%)]\tLoss: 0.057011\n",
      "Train Epoch: 1 [49280/51005 (97%)]\tLoss: 0.023941\n",
      "Train Epoch: 1 [49920/51005 (98%)]\tLoss: 0.015270\n",
      "Train Epoch: 1 [50560/51005 (99%)]\tLoss: 0.112994\n",
      "\n",
      "Accuracy: 8839/8995 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/51005 (0%)]\tLoss: 0.030147\n",
      "Train Epoch: 2 [640/51005 (1%)]\tLoss: 0.097551\n",
      "Train Epoch: 2 [1280/51005 (3%)]\tLoss: 0.015605\n",
      "Train Epoch: 2 [1920/51005 (4%)]\tLoss: 0.063764\n",
      "Train Epoch: 2 [2560/51005 (5%)]\tLoss: 0.016529\n",
      "Train Epoch: 2 [3200/51005 (6%)]\tLoss: 0.029704\n",
      "Train Epoch: 2 [3840/51005 (8%)]\tLoss: 0.069271\n",
      "Train Epoch: 2 [4480/51005 (9%)]\tLoss: 0.018267\n",
      "Train Epoch: 2 [5120/51005 (10%)]\tLoss: 0.118079\n",
      "Train Epoch: 2 [5760/51005 (11%)]\tLoss: 0.064476\n",
      "Train Epoch: 2 [6400/51005 (13%)]\tLoss: 0.021223\n",
      "Train Epoch: 2 [7040/51005 (14%)]\tLoss: 0.060058\n",
      "Train Epoch: 2 [7680/51005 (15%)]\tLoss: 0.099531\n",
      "Train Epoch: 2 [8320/51005 (16%)]\tLoss: 0.015765\n",
      "Train Epoch: 2 [8960/51005 (18%)]\tLoss: 0.053097\n",
      "Train Epoch: 2 [9600/51005 (19%)]\tLoss: 0.042598\n",
      "Train Epoch: 2 [10240/51005 (20%)]\tLoss: 0.053649\n",
      "Train Epoch: 2 [10880/51005 (21%)]\tLoss: 0.060426\n",
      "Train Epoch: 2 [11520/51005 (23%)]\tLoss: 0.085964\n",
      "Train Epoch: 2 [12160/51005 (24%)]\tLoss: 0.054256\n",
      "Train Epoch: 2 [12800/51005 (25%)]\tLoss: 0.193773\n",
      "Train Epoch: 2 [13440/51005 (26%)]\tLoss: 0.016227\n",
      "Train Epoch: 2 [14080/51005 (28%)]\tLoss: 0.009884\n",
      "Train Epoch: 2 [14720/51005 (29%)]\tLoss: 0.059157\n",
      "Train Epoch: 2 [15360/51005 (30%)]\tLoss: 0.040582\n",
      "Train Epoch: 2 [16000/51005 (31%)]\tLoss: 0.064086\n",
      "Train Epoch: 2 [16640/51005 (33%)]\tLoss: 0.017255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [17280/51005 (34%)]\tLoss: 0.017924\n",
      "Train Epoch: 2 [17920/51005 (35%)]\tLoss: 0.012809\n",
      "Train Epoch: 2 [18560/51005 (36%)]\tLoss: 0.053179\n",
      "Train Epoch: 2 [19200/51005 (38%)]\tLoss: 0.008463\n",
      "Train Epoch: 2 [19840/51005 (39%)]\tLoss: 0.093633\n",
      "Train Epoch: 2 [20480/51005 (40%)]\tLoss: 0.087926\n",
      "Train Epoch: 2 [21120/51005 (41%)]\tLoss: 0.036048\n",
      "Train Epoch: 2 [21760/51005 (43%)]\tLoss: 0.005182\n",
      "Train Epoch: 2 [22400/51005 (44%)]\tLoss: 0.073555\n",
      "Train Epoch: 2 [23040/51005 (45%)]\tLoss: 0.040741\n",
      "Train Epoch: 2 [23680/51005 (46%)]\tLoss: 0.003862\n",
      "Train Epoch: 2 [24320/51005 (48%)]\tLoss: 0.074093\n",
      "Train Epoch: 2 [24960/51005 (49%)]\tLoss: 0.012367\n",
      "Train Epoch: 2 [25600/51005 (50%)]\tLoss: 0.007210\n",
      "Train Epoch: 2 [26240/51005 (51%)]\tLoss: 0.036015\n",
      "Train Epoch: 2 [26880/51005 (53%)]\tLoss: 0.063972\n",
      "Train Epoch: 2 [27520/51005 (54%)]\tLoss: 0.040846\n",
      "Train Epoch: 2 [28160/51005 (55%)]\tLoss: 0.302554\n",
      "Train Epoch: 2 [28800/51005 (56%)]\tLoss: 0.016676\n",
      "Train Epoch: 2 [29440/51005 (58%)]\tLoss: 0.045967\n",
      "Train Epoch: 2 [30080/51005 (59%)]\tLoss: 0.005578\n",
      "Train Epoch: 2 [30720/51005 (60%)]\tLoss: 0.089029\n",
      "Train Epoch: 2 [31360/51005 (61%)]\tLoss: 0.046817\n",
      "Train Epoch: 2 [32000/51005 (63%)]\tLoss: 0.002341\n",
      "Train Epoch: 2 [32640/51005 (64%)]\tLoss: 0.008782\n",
      "Train Epoch: 2 [33280/51005 (65%)]\tLoss: 0.021506\n",
      "Train Epoch: 2 [33920/51005 (66%)]\tLoss: 0.007861\n",
      "Train Epoch: 2 [34560/51005 (68%)]\tLoss: 0.140241\n",
      "Train Epoch: 2 [35200/51005 (69%)]\tLoss: 0.020487\n",
      "Train Epoch: 2 [35840/51005 (70%)]\tLoss: 0.028355\n",
      "Train Epoch: 2 [36480/51005 (72%)]\tLoss: 0.003981\n",
      "Train Epoch: 2 [37120/51005 (73%)]\tLoss: 0.065124\n",
      "Train Epoch: 2 [37760/51005 (74%)]\tLoss: 0.029422\n",
      "Train Epoch: 2 [38400/51005 (75%)]\tLoss: 0.052685\n",
      "Train Epoch: 2 [39040/51005 (77%)]\tLoss: 0.005808\n",
      "Train Epoch: 2 [39680/51005 (78%)]\tLoss: 0.006589\n",
      "Train Epoch: 2 [40320/51005 (79%)]\tLoss: 0.082366\n",
      "Train Epoch: 2 [40960/51005 (80%)]\tLoss: 0.017523\n",
      "Train Epoch: 2 [41600/51005 (82%)]\tLoss: 0.009534\n",
      "Train Epoch: 2 [42240/51005 (83%)]\tLoss: 0.004814\n",
      "Train Epoch: 2 [42880/51005 (84%)]\tLoss: 0.184732\n",
      "Train Epoch: 2 [43520/51005 (85%)]\tLoss: 0.000809\n",
      "Train Epoch: 2 [44160/51005 (87%)]\tLoss: 0.030575\n",
      "Train Epoch: 2 [44800/51005 (88%)]\tLoss: 0.024352\n",
      "Train Epoch: 2 [45440/51005 (89%)]\tLoss: 0.026768\n",
      "Train Epoch: 2 [46080/51005 (90%)]\tLoss: 0.030256\n",
      "Train Epoch: 2 [46720/51005 (92%)]\tLoss: 0.004746\n",
      "Train Epoch: 2 [47360/51005 (93%)]\tLoss: 0.017243\n",
      "Train Epoch: 2 [48000/51005 (94%)]\tLoss: 0.001183\n",
      "Train Epoch: 2 [48640/51005 (95%)]\tLoss: 0.026312\n",
      "Train Epoch: 2 [49280/51005 (97%)]\tLoss: 0.029872\n",
      "Train Epoch: 2 [49920/51005 (98%)]\tLoss: 0.045953\n",
      "Train Epoch: 2 [50560/51005 (99%)]\tLoss: 0.008321\n",
      "\n",
      "Accuracy: 8895/8995 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/51005 (0%)]\tLoss: 0.073215\n",
      "Train Epoch: 3 [640/51005 (1%)]\tLoss: 0.008645\n",
      "Train Epoch: 3 [1280/51005 (3%)]\tLoss: 0.035278\n",
      "Train Epoch: 3 [1920/51005 (4%)]\tLoss: 0.002936\n",
      "Train Epoch: 3 [2560/51005 (5%)]\tLoss: 0.016993\n",
      "Train Epoch: 3 [3200/51005 (6%)]\tLoss: 0.057631\n",
      "Train Epoch: 3 [3840/51005 (8%)]\tLoss: 0.090594\n",
      "Train Epoch: 3 [4480/51005 (9%)]\tLoss: 0.018480\n",
      "Train Epoch: 3 [5120/51005 (10%)]\tLoss: 0.004982\n",
      "Train Epoch: 3 [5760/51005 (11%)]\tLoss: 0.001692\n",
      "Train Epoch: 3 [6400/51005 (13%)]\tLoss: 0.078646\n",
      "Train Epoch: 3 [7040/51005 (14%)]\tLoss: 0.005741\n",
      "Train Epoch: 3 [7680/51005 (15%)]\tLoss: 0.007715\n",
      "Train Epoch: 3 [8320/51005 (16%)]\tLoss: 0.018638\n",
      "Train Epoch: 3 [8960/51005 (18%)]\tLoss: 0.006345\n",
      "Train Epoch: 3 [9600/51005 (19%)]\tLoss: 0.067542\n",
      "Train Epoch: 3 [10240/51005 (20%)]\tLoss: 0.006051\n",
      "Train Epoch: 3 [10880/51005 (21%)]\tLoss: 0.052641\n",
      "Train Epoch: 3 [11520/51005 (23%)]\tLoss: 0.033421\n",
      "Train Epoch: 3 [12160/51005 (24%)]\tLoss: 0.012564\n",
      "Train Epoch: 3 [12800/51005 (25%)]\tLoss: 0.001320\n",
      "Train Epoch: 3 [13440/51005 (26%)]\tLoss: 0.026304\n",
      "Train Epoch: 3 [14080/51005 (28%)]\tLoss: 0.017112\n",
      "Train Epoch: 3 [14720/51005 (29%)]\tLoss: 0.028465\n",
      "Train Epoch: 3 [15360/51005 (30%)]\tLoss: 0.018104\n",
      "Train Epoch: 3 [16000/51005 (31%)]\tLoss: 0.020270\n",
      "Train Epoch: 3 [16640/51005 (33%)]\tLoss: 0.028487\n",
      "Train Epoch: 3 [17280/51005 (34%)]\tLoss: 0.065175\n",
      "Train Epoch: 3 [17920/51005 (35%)]\tLoss: 0.002519\n",
      "Train Epoch: 3 [18560/51005 (36%)]\tLoss: 0.059114\n",
      "Train Epoch: 3 [19200/51005 (38%)]\tLoss: 0.014212\n",
      "Train Epoch: 3 [19840/51005 (39%)]\tLoss: 0.075049\n",
      "Train Epoch: 3 [20480/51005 (40%)]\tLoss: 0.045264\n",
      "Train Epoch: 3 [21120/51005 (41%)]\tLoss: 0.000703\n",
      "Train Epoch: 3 [21760/51005 (43%)]\tLoss: 0.017703\n",
      "Train Epoch: 3 [22400/51005 (44%)]\tLoss: 0.014912\n",
      "Train Epoch: 3 [23040/51005 (45%)]\tLoss: 0.001375\n",
      "Train Epoch: 3 [23680/51005 (46%)]\tLoss: 0.014383\n",
      "Train Epoch: 3 [24320/51005 (48%)]\tLoss: 0.013519\n",
      "Train Epoch: 3 [24960/51005 (49%)]\tLoss: 0.008254\n",
      "Train Epoch: 3 [25600/51005 (50%)]\tLoss: 0.003543\n",
      "Train Epoch: 3 [26240/51005 (51%)]\tLoss: 0.007723\n",
      "Train Epoch: 3 [26880/51005 (53%)]\tLoss: 0.013228\n",
      "Train Epoch: 3 [27520/51005 (54%)]\tLoss: 0.001567\n",
      "Train Epoch: 3 [28160/51005 (55%)]\tLoss: 0.019835\n",
      "Train Epoch: 3 [28800/51005 (56%)]\tLoss: 0.005701\n",
      "Train Epoch: 3 [29440/51005 (58%)]\tLoss: 0.096539\n",
      "Train Epoch: 3 [30080/51005 (59%)]\tLoss: 0.001349\n",
      "Train Epoch: 3 [30720/51005 (60%)]\tLoss: 0.010301\n",
      "Train Epoch: 3 [31360/51005 (61%)]\tLoss: 0.100033\n",
      "Train Epoch: 3 [32000/51005 (63%)]\tLoss: 0.035164\n",
      "Train Epoch: 3 [32640/51005 (64%)]\tLoss: 0.130693\n",
      "Train Epoch: 3 [33280/51005 (65%)]\tLoss: 0.015835\n",
      "Train Epoch: 3 [33920/51005 (66%)]\tLoss: 0.025887\n",
      "Train Epoch: 3 [34560/51005 (68%)]\tLoss: 0.033844\n",
      "Train Epoch: 3 [35200/51005 (69%)]\tLoss: 0.043425\n",
      "Train Epoch: 3 [35840/51005 (70%)]\tLoss: 0.014571\n",
      "Train Epoch: 3 [36480/51005 (72%)]\tLoss: 0.012071\n",
      "Train Epoch: 3 [37120/51005 (73%)]\tLoss: 0.168766\n",
      "Train Epoch: 3 [37760/51005 (74%)]\tLoss: 0.035784\n",
      "Train Epoch: 3 [38400/51005 (75%)]\tLoss: 0.036587\n",
      "Train Epoch: 3 [39040/51005 (77%)]\tLoss: 0.047739\n",
      "Train Epoch: 3 [39680/51005 (78%)]\tLoss: 0.019373\n",
      "Train Epoch: 3 [40320/51005 (79%)]\tLoss: 0.010239\n",
      "Train Epoch: 3 [40960/51005 (80%)]\tLoss: 0.001260\n",
      "Train Epoch: 3 [41600/51005 (82%)]\tLoss: 0.008208\n",
      "Train Epoch: 3 [42240/51005 (83%)]\tLoss: 0.016462\n",
      "Train Epoch: 3 [42880/51005 (84%)]\tLoss: 0.096797\n",
      "Train Epoch: 3 [43520/51005 (85%)]\tLoss: 0.009192\n",
      "Train Epoch: 3 [44160/51005 (87%)]\tLoss: 0.004031\n",
      "Train Epoch: 3 [44800/51005 (88%)]\tLoss: 0.007989\n",
      "Train Epoch: 3 [45440/51005 (89%)]\tLoss: 0.033866\n",
      "Train Epoch: 3 [46080/51005 (90%)]\tLoss: 0.036146\n",
      "Train Epoch: 3 [46720/51005 (92%)]\tLoss: 0.085556\n",
      "Train Epoch: 3 [47360/51005 (93%)]\tLoss: 0.059535\n",
      "Train Epoch: 3 [48000/51005 (94%)]\tLoss: 0.047762\n",
      "Train Epoch: 3 [48640/51005 (95%)]\tLoss: 0.007266\n",
      "Train Epoch: 3 [49280/51005 (97%)]\tLoss: 0.056683\n",
      "Train Epoch: 3 [49920/51005 (98%)]\tLoss: 0.006219\n",
      "Train Epoch: 3 [50560/51005 (99%)]\tLoss: 0.011909\n",
      "\n",
      "Accuracy: 8907/8995 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/51005 (0%)]\tLoss: 0.018108\n",
      "Train Epoch: 4 [640/51005 (1%)]\tLoss: 0.093752\n",
      "Train Epoch: 4 [1280/51005 (3%)]\tLoss: 0.005837\n",
      "Train Epoch: 4 [1920/51005 (4%)]\tLoss: 0.016539\n",
      "Train Epoch: 4 [2560/51005 (5%)]\tLoss: 0.016962\n",
      "Train Epoch: 4 [3200/51005 (6%)]\tLoss: 0.011619\n",
      "Train Epoch: 4 [3840/51005 (8%)]\tLoss: 0.014729\n",
      "Train Epoch: 4 [4480/51005 (9%)]\tLoss: 0.010575\n",
      "Train Epoch: 4 [5120/51005 (10%)]\tLoss: 0.004102\n",
      "Train Epoch: 4 [5760/51005 (11%)]\tLoss: 0.035280\n",
      "Train Epoch: 4 [6400/51005 (13%)]\tLoss: 0.010920\n",
      "Train Epoch: 4 [7040/51005 (14%)]\tLoss: 0.023125\n",
      "Train Epoch: 4 [7680/51005 (15%)]\tLoss: 0.004273\n",
      "Train Epoch: 4 [8320/51005 (16%)]\tLoss: 0.003162\n",
      "Train Epoch: 4 [8960/51005 (18%)]\tLoss: 0.001512\n",
      "Train Epoch: 4 [9600/51005 (19%)]\tLoss: 0.032378\n",
      "Train Epoch: 4 [10240/51005 (20%)]\tLoss: 0.004446\n",
      "Train Epoch: 4 [10880/51005 (21%)]\tLoss: 0.001630\n",
      "Train Epoch: 4 [11520/51005 (23%)]\tLoss: 0.012219\n",
      "Train Epoch: 4 [12160/51005 (24%)]\tLoss: 0.005602\n",
      "Train Epoch: 4 [12800/51005 (25%)]\tLoss: 0.066619\n",
      "Train Epoch: 4 [13440/51005 (26%)]\tLoss: 0.014355\n",
      "Train Epoch: 4 [14080/51005 (28%)]\tLoss: 0.162211\n",
      "Train Epoch: 4 [14720/51005 (29%)]\tLoss: 0.001801\n",
      "Train Epoch: 4 [15360/51005 (30%)]\tLoss: 0.027753\n",
      "Train Epoch: 4 [16000/51005 (31%)]\tLoss: 0.004790\n",
      "Train Epoch: 4 [16640/51005 (33%)]\tLoss: 0.017066\n",
      "Train Epoch: 4 [17280/51005 (34%)]\tLoss: 0.003118\n",
      "Train Epoch: 4 [17920/51005 (35%)]\tLoss: 0.021535\n",
      "Train Epoch: 4 [18560/51005 (36%)]\tLoss: 0.017188\n",
      "Train Epoch: 4 [19200/51005 (38%)]\tLoss: 0.011610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [19840/51005 (39%)]\tLoss: 0.002982\n",
      "Train Epoch: 4 [20480/51005 (40%)]\tLoss: 0.068850\n",
      "Train Epoch: 4 [21120/51005 (41%)]\tLoss: 0.052400\n",
      "Train Epoch: 4 [21760/51005 (43%)]\tLoss: 0.024690\n",
      "Train Epoch: 4 [22400/51005 (44%)]\tLoss: 0.001547\n",
      "Train Epoch: 4 [23040/51005 (45%)]\tLoss: 0.059049\n",
      "Train Epoch: 4 [23680/51005 (46%)]\tLoss: 0.010651\n",
      "Train Epoch: 4 [24320/51005 (48%)]\tLoss: 0.123506\n",
      "Train Epoch: 4 [24960/51005 (49%)]\tLoss: 0.090056\n",
      "Train Epoch: 4 [25600/51005 (50%)]\tLoss: 0.011373\n",
      "Train Epoch: 4 [26240/51005 (51%)]\tLoss: 0.006037\n",
      "Train Epoch: 4 [26880/51005 (53%)]\tLoss: 0.015710\n",
      "Train Epoch: 4 [27520/51005 (54%)]\tLoss: 0.010152\n",
      "Train Epoch: 4 [28160/51005 (55%)]\tLoss: 0.001200\n",
      "Train Epoch: 4 [28800/51005 (56%)]\tLoss: 0.062949\n",
      "Train Epoch: 4 [29440/51005 (58%)]\tLoss: 0.017173\n",
      "Train Epoch: 4 [30080/51005 (59%)]\tLoss: 0.003513\n",
      "Train Epoch: 4 [30720/51005 (60%)]\tLoss: 0.001245\n",
      "Train Epoch: 4 [31360/51005 (61%)]\tLoss: 0.005846\n",
      "Train Epoch: 4 [32000/51005 (63%)]\tLoss: 0.003876\n",
      "Train Epoch: 4 [32640/51005 (64%)]\tLoss: 0.000597\n",
      "Train Epoch: 4 [33280/51005 (65%)]\tLoss: 0.065753\n",
      "Train Epoch: 4 [33920/51005 (66%)]\tLoss: 0.071937\n",
      "Train Epoch: 4 [34560/51005 (68%)]\tLoss: 0.049673\n",
      "Train Epoch: 4 [35200/51005 (69%)]\tLoss: 0.001033\n",
      "Train Epoch: 4 [35840/51005 (70%)]\tLoss: 0.002576\n",
      "Train Epoch: 4 [36480/51005 (72%)]\tLoss: 0.057814\n",
      "Train Epoch: 4 [37120/51005 (73%)]\tLoss: 0.049138\n",
      "Train Epoch: 4 [37760/51005 (74%)]\tLoss: 0.003536\n",
      "Train Epoch: 4 [38400/51005 (75%)]\tLoss: 0.005310\n",
      "Train Epoch: 4 [39040/51005 (77%)]\tLoss: 0.003478\n",
      "Train Epoch: 4 [39680/51005 (78%)]\tLoss: 0.139625\n",
      "Train Epoch: 4 [40320/51005 (79%)]\tLoss: 0.025119\n",
      "Train Epoch: 4 [40960/51005 (80%)]\tLoss: 0.089596\n",
      "Train Epoch: 4 [41600/51005 (82%)]\tLoss: 0.002059\n",
      "Train Epoch: 4 [42240/51005 (83%)]\tLoss: 0.089023\n",
      "Train Epoch: 4 [42880/51005 (84%)]\tLoss: 0.000363\n",
      "Train Epoch: 4 [43520/51005 (85%)]\tLoss: 0.025474\n",
      "Train Epoch: 4 [44160/51005 (87%)]\tLoss: 0.001110\n",
      "Train Epoch: 4 [44800/51005 (88%)]\tLoss: 0.003474\n",
      "Train Epoch: 4 [45440/51005 (89%)]\tLoss: 0.008297\n",
      "Train Epoch: 4 [46080/51005 (90%)]\tLoss: 0.009645\n",
      "Train Epoch: 4 [46720/51005 (92%)]\tLoss: 0.149207\n",
      "Train Epoch: 4 [47360/51005 (93%)]\tLoss: 0.035727\n",
      "Train Epoch: 4 [48000/51005 (94%)]\tLoss: 0.061090\n",
      "Train Epoch: 4 [48640/51005 (95%)]\tLoss: 0.003595\n",
      "Train Epoch: 4 [49280/51005 (97%)]\tLoss: 0.001521\n",
      "Train Epoch: 4 [49920/51005 (98%)]\tLoss: 0.002308\n",
      "Train Epoch: 4 [50560/51005 (99%)]\tLoss: 0.011381\n",
      "\n",
      "Accuracy: 8910/8995 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/51005 (0%)]\tLoss: 0.008651\n",
      "Train Epoch: 5 [640/51005 (1%)]\tLoss: 0.007754\n",
      "Train Epoch: 5 [1280/51005 (3%)]\tLoss: 0.010599\n",
      "Train Epoch: 5 [1920/51005 (4%)]\tLoss: 0.054955\n",
      "Train Epoch: 5 [2560/51005 (5%)]\tLoss: 0.000396\n",
      "Train Epoch: 5 [3200/51005 (6%)]\tLoss: 0.014121\n",
      "Train Epoch: 5 [3840/51005 (8%)]\tLoss: 0.002303\n",
      "Train Epoch: 5 [4480/51005 (9%)]\tLoss: 0.010872\n",
      "Train Epoch: 5 [5120/51005 (10%)]\tLoss: 0.041557\n",
      "Train Epoch: 5 [5760/51005 (11%)]\tLoss: 0.004068\n",
      "Train Epoch: 5 [6400/51005 (13%)]\tLoss: 0.002570\n",
      "Train Epoch: 5 [7040/51005 (14%)]\tLoss: 0.001278\n",
      "Train Epoch: 5 [7680/51005 (15%)]\tLoss: 0.062850\n",
      "Train Epoch: 5 [8320/51005 (16%)]\tLoss: 0.005514\n",
      "Train Epoch: 5 [8960/51005 (18%)]\tLoss: 0.037938\n",
      "Train Epoch: 5 [9600/51005 (19%)]\tLoss: 0.006112\n",
      "Train Epoch: 5 [10240/51005 (20%)]\tLoss: 0.040895\n",
      "Train Epoch: 5 [10880/51005 (21%)]\tLoss: 0.001664\n",
      "Train Epoch: 5 [11520/51005 (23%)]\tLoss: 0.012613\n",
      "Train Epoch: 5 [12160/51005 (24%)]\tLoss: 0.004914\n",
      "Train Epoch: 5 [12800/51005 (25%)]\tLoss: 0.014947\n",
      "Train Epoch: 5 [13440/51005 (26%)]\tLoss: 0.007447\n",
      "Train Epoch: 5 [14080/51005 (28%)]\tLoss: 0.062559\n",
      "Train Epoch: 5 [14720/51005 (29%)]\tLoss: 0.001560\n",
      "Train Epoch: 5 [15360/51005 (30%)]\tLoss: 0.000746\n",
      "Train Epoch: 5 [16000/51005 (31%)]\tLoss: 0.002766\n",
      "Train Epoch: 5 [16640/51005 (33%)]\tLoss: 0.030605\n",
      "Train Epoch: 5 [17280/51005 (34%)]\tLoss: 0.002624\n",
      "Train Epoch: 5 [17920/51005 (35%)]\tLoss: 0.008617\n",
      "Train Epoch: 5 [18560/51005 (36%)]\tLoss: 0.000872\n",
      "Train Epoch: 5 [19200/51005 (38%)]\tLoss: 0.026682\n",
      "Train Epoch: 5 [19840/51005 (39%)]\tLoss: 0.001605\n",
      "Train Epoch: 5 [20480/51005 (40%)]\tLoss: 0.004311\n",
      "Train Epoch: 5 [21120/51005 (41%)]\tLoss: 0.004356\n",
      "Train Epoch: 5 [21760/51005 (43%)]\tLoss: 0.001831\n",
      "Train Epoch: 5 [22400/51005 (44%)]\tLoss: 0.135277\n",
      "Train Epoch: 5 [23040/51005 (45%)]\tLoss: 0.007187\n",
      "Train Epoch: 5 [23680/51005 (46%)]\tLoss: 0.000950\n",
      "Train Epoch: 5 [24320/51005 (48%)]\tLoss: 0.004627\n",
      "Train Epoch: 5 [24960/51005 (49%)]\tLoss: 0.006267\n",
      "Train Epoch: 5 [25600/51005 (50%)]\tLoss: 0.057551\n",
      "Train Epoch: 5 [26240/51005 (51%)]\tLoss: 0.003719\n",
      "Train Epoch: 5 [26880/51005 (53%)]\tLoss: 0.027941\n",
      "Train Epoch: 5 [27520/51005 (54%)]\tLoss: 0.011782\n",
      "Train Epoch: 5 [28160/51005 (55%)]\tLoss: 0.005591\n",
      "Train Epoch: 5 [28800/51005 (56%)]\tLoss: 0.057660\n",
      "Train Epoch: 5 [29440/51005 (58%)]\tLoss: 0.003977\n",
      "Train Epoch: 5 [30080/51005 (59%)]\tLoss: 0.031242\n",
      "Train Epoch: 5 [30720/51005 (60%)]\tLoss: 0.009301\n",
      "Train Epoch: 5 [31360/51005 (61%)]\tLoss: 0.004377\n",
      "Train Epoch: 5 [32000/51005 (63%)]\tLoss: 0.029127\n",
      "Train Epoch: 5 [32640/51005 (64%)]\tLoss: 0.007786\n",
      "Train Epoch: 5 [33280/51005 (65%)]\tLoss: 0.001624\n",
      "Train Epoch: 5 [33920/51005 (66%)]\tLoss: 0.010160\n",
      "Train Epoch: 5 [34560/51005 (68%)]\tLoss: 0.015023\n",
      "Train Epoch: 5 [35200/51005 (69%)]\tLoss: 0.013772\n",
      "Train Epoch: 5 [35840/51005 (70%)]\tLoss: 0.002578\n",
      "Train Epoch: 5 [36480/51005 (72%)]\tLoss: 0.001128\n",
      "Train Epoch: 5 [37120/51005 (73%)]\tLoss: 0.058313\n",
      "Train Epoch: 5 [37760/51005 (74%)]\tLoss: 0.010557\n",
      "Train Epoch: 5 [38400/51005 (75%)]\tLoss: 0.000866\n",
      "Train Epoch: 5 [39040/51005 (77%)]\tLoss: 0.013666\n",
      "Train Epoch: 5 [39680/51005 (78%)]\tLoss: 0.000752\n",
      "Train Epoch: 5 [40320/51005 (79%)]\tLoss: 0.009558\n",
      "Train Epoch: 5 [40960/51005 (80%)]\tLoss: 0.003832\n",
      "Train Epoch: 5 [41600/51005 (82%)]\tLoss: 0.018355\n",
      "Train Epoch: 5 [42240/51005 (83%)]\tLoss: 0.000702\n",
      "Train Epoch: 5 [42880/51005 (84%)]\tLoss: 0.000191\n",
      "Train Epoch: 5 [43520/51005 (85%)]\tLoss: 0.000917\n",
      "Train Epoch: 5 [44160/51005 (87%)]\tLoss: 0.002156\n",
      "Train Epoch: 5 [44800/51005 (88%)]\tLoss: 0.002968\n",
      "Train Epoch: 5 [45440/51005 (89%)]\tLoss: 0.003438\n",
      "Train Epoch: 5 [46080/51005 (90%)]\tLoss: 0.000399\n",
      "Train Epoch: 5 [46720/51005 (92%)]\tLoss: 0.000418\n",
      "Train Epoch: 5 [47360/51005 (93%)]\tLoss: 0.116644\n",
      "Train Epoch: 5 [48000/51005 (94%)]\tLoss: 0.029203\n",
      "Train Epoch: 5 [48640/51005 (95%)]\tLoss: 0.000636\n",
      "Train Epoch: 5 [49280/51005 (97%)]\tLoss: 0.001377\n",
      "Train Epoch: 5 [49920/51005 (98%)]\tLoss: 0.000872\n",
      "Train Epoch: 5 [50560/51005 (99%)]\tLoss: 0.009655\n",
      "\n",
      "Accuracy: 8919/8995 (99%)\n",
      "\n",
      "Training Set (1 Subset):\n",
      "\n",
      "Accuracy: 50843/51005 (100%)\n",
      "\n",
      "Test Set (1 Subset):\n",
      "\n",
      "Accuracy: 9925/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subsets = [0.0625, 0.125, 0.25, 0.5, 1]\n",
    "\n",
    "training_losses = []\n",
    "valid_losses = []\n",
    "training_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for fraction in subsets:\n",
    "    subset_indices_train, subset_indices_valid = train_valid_split(train_dataset, fraction)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64,\n",
    "        sampler=SubsetRandomSampler(subset_indices_train)\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64,\n",
    "        sampler=SubsetRandomSampler(subset_indices_valid)\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    model = Net()\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    epochs = 5\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss.append(train(model, train_loader, optimizer, epoch))\n",
    "        valid_loss.append(test(model, val_loader))\n",
    "        scheduler.step() \n",
    "\n",
    "    print(\"Training Set ({} Subset):\".format(str(fraction)))\n",
    "    train_error = test(model, train_loader)\n",
    "    print(\"Test Set ({} Subset):\".format(str(fraction)))\n",
    "    test_error = test(model, test_loader)\n",
    "    \n",
    "    training_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    training_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    torch.save(model.state_dict(), \"best_MNIST_Net_{}.pt\".format(str(fraction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_samples = [60000 * element for element in subsets]\n",
    "\n",
    "def calculate_error(accuracy_values):\n",
    "    error = []\n",
    "    for val in accuracy_values:\n",
    "        error.append(100. - val)\n",
    "    return error\n",
    "\n",
    "training_errors = calculate_error(training_errors)\n",
    "test_errors = calculate_error(test_errors)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 10))\n",
    "plt.scatter(np.log(training_samples), np.log(training_errors), label = \"Train\")\n",
    "plt.scatter(np.log(training_samples), np.log(test_errors), label = \"Test\")\n",
    "\n",
    "plt.xlabel(\"Ln(Number of Training Samples)\")\n",
    "plt.ylabel(\"Ln(Error)\")\n",
    "fig.suptitle(\"TRAIN AND TEST ERROR\")\n",
    "plt.legend()\n",
    "plt.savefig(\"train_and_test_error.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
